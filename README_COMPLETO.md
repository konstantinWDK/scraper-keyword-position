# üîç Keyword Position Scraper - Anti-detecci√≥n 2025

## üìã Tabla de Contenidos
- [Descripci√≥n](#-descripci√≥n)
- [Caracter√≠sticas](#-caracter√≠sticas)
- [Instalaci√≥n](#-instalaci√≥n)
- [Configuraci√≥n](#-configuraci√≥n)
- [Uso](#-uso)
- [Interfaz Gr√°fica](#-interfaz-gr√°fica)
- [API](#-api)
- [Troubleshooting](#-troubleshooting)
- [FAQ](#-faq)
- [Contribuci√≥n](#-contribuci√≥n)
- [Licencia](#-licencia)

## üéØ Descripci√≥n

Scraper avanzado de posiciones de keywords en Google SERP con t√©cnicas de anti-detecci√≥n avanzadas. Dise√±ado para SEOs, marketers y desarrolladores que necesitan monitorear posiciones de manera eficiente y discreta.

**Alternativa gratuita a herramientas premium como ScrapeBox, Ahrefs y SEMrush.**

## ‚ú® Caracter√≠sticas

### üõ°Ô∏è Anti-detecci√≥n Avanzada
- **Undetected ChromeDriver** - Navegador indetectable
- **Headers rotativos** - User-agents y headers realistas
- **Comportamiento humano** - Delays variables, scroll aleatorio, movimientos de mouse
- **JavaScript anti-fingerprinting** - Scripts para evadir detecci√≥n
- **Window size variable** - Resoluciones aleatorias

### üîÑ Gesti√≥n de Proxies
- **Rotaci√≥n autom√°tica** - Cambio inteligente entre proxies
- **Soporte m√∫ltiple** - HTTP/HTTPS, con/sin autenticaci√≥n
- **Testing autom√°tico** - Verificaci√≥n de proxies funcionales
- **Fallback seguro** - Continuaci√≥n sin proxy si es necesario

### üîç Funcionalidades de Scraping
- **SERP scraping** - Extracci√≥n completa de resultados de b√∫squeda
- **Google Suggest** - Generaci√≥n autom√°tica de keywords
- **B√∫squeda por dominio** - Tracking espec√≠fico de competidores
- **M√∫ltiples p√°ginas** - Scraping de hasta 10 p√°ginas por keyword
- **An√°lisis en tiempo real** - Estad√≠sticas y m√©tricas inmediatas

### üìä An√°lisis y Exportaci√≥n
- **M√∫ltiples formatos** - CSV, JSON, an√°lisis estad√≠stico
- **Dashboard visual** - Gr√°ficos y m√©tricas interactivas
- **Comparativas** - Evoluci√≥n de posiciones en el tiempo
- **Alertas autom√°ticas** - Notificaciones de cambios importantes

## üöÄ Instalaci√≥n

### Prerrequisitos
- Python 3.8+
- Google Chrome instalado
- Conexi√≥n a internet
- (Opcional) Proxies para scraping masivo

### Instalaci√≥n Autom√°tica (Recomendada)

```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/scraper-keyword-position.git
cd scraper-keyword-position

# Ejecutar script de instalaci√≥n
chmod +x install.sh
./install.sh
```

### Instalaci√≥n Manual

```bash
# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Configurar entorno
cp config/.env.example config/.env
```

### Verificar Instalaci√≥n

```bash
# Probar instalaci√≥n
python src/main.py --test

# Verificar configuraci√≥n
python src/main.py --config
```

## ‚öôÔ∏è Configuraci√≥n

### Archivo de Configuraci√≥n (.env)

Edita `config/.env` con tus preferencias:

```env
# ========== PROXIES ==========
# Formato: usuario:password@ip:puerto o ip:puerto
PROXY_1=user:pass@proxy1.com:8080
PROXY_2=proxy2.com:3128
PROXY_3=192.168.1.100:8080

# ========== DELAYS ==========
# Tiempos en segundos entre requests
MIN_KEYWORD_DELAY=5
MAX_KEYWORD_DELAY=15
MIN_PAGE_DELAY=3
MAX_PAGE_DELAY=8

# ========== GOOGLE ==========
DEFAULT_COUNTRY=US
DEFAULT_LANGUAGE=en
PAGES_TO_SCRAPE=1

# ========== LOGS ==========
LOG_LEVEL=INFO

# ========== RESULTADOS ==========
SAVE_JSON=true
SAVE_CSV=true

# ========== USER AGENTS ==========
CUSTOM_USER_AGENTS=
# Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
# Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36
```

### Configuraci√≥n Recomendada

| Escenario | Min Delay | Max Delay | Proxies | P√°ginas |
|-----------|-----------|-----------|---------|---------|
| **Testing** | 3s | 8s | 0-1 | 1 |
| **Producci√≥n** | 5s | 15s | 3-5 | 1-2 |
| **Masivo** | 8s | 20s | 10+ | 1 |

## üîß Uso

### Interfaz de L√≠nea de Comandos

#### Comandos B√°sicos

```bash
# Modo de prueba
python src/main.py --test

# Keywords espec√≠ficas
python src/main.py --keywords "seo,marketing digital" --domain example.com

# Desde archivo
python src/main.py --keyword-file keywords.txt --domain example.com --pages 2

# Google Suggest
python src/main.py --suggest "marketing" --country ES --language es

# An√°lisis de resultados
python src/main.py --analyze
```

#### Comandos Avanzados

```bash
# Scraping m√∫ltiples p√°ginas con salida personalizada
python src/main.py --keywords "seo,sem,marketing" --domain midominio.com --pages 3 --output mis_resultados

# Procesamiento por lotes
python src/main.py --batch keywords.txt --domain ejemplo.com --country ES --language es

# Solo generar keywords (sin scraping)
python src/main.py --suggest "marketing digital" --country ES --no-scrape

# An√°lisis de archivo espec√≠fico
python src/main.py --analyze-file data/positions_20241201_143022.csv
```

### Interfaz Gr√°fica (GUI)

```bash
# Iniciar interfaz gr√°fica
python src/gui.py
```

La GUI ofrece:
- **Configuraci√≥n visual** - Ajustes con interfaz intuitiva
- **Gesti√≥n de keywords** - Carga y edici√≥n visual
- **Monitoreo en tiempo real** - Progreso y estad√≠sticas
- **Visualizaci√≥n de resultados** - Gr√°ficos y tablas interactivas
- **Exportaci√≥n f√°cil** - Botones de exportaci√≥n directa

## üñ•Ô∏è Interfaz Gr√°fica

### Caracter√≠sticas de la GUI

1. **Panel de Configuraci√≥n**
   - Ajustes de delays y proxies
   - Configuraci√≥n de pa√≠s e idioma
   - Opciones de exportaci√≥n

2. **Gestor de Keywords**
   - Carga desde archivo
   - Edici√≥n directa
   - Generaci√≥n con Google Suggest

3. **Monitor de Progreso**
   - Barra de progreso en tiempo real
   - Estad√≠sticas actualizadas
   - Logs de actividad

4. **Visualizador de Resultados**
   - Tablas interactivas
   - Gr√°ficos de distribuci√≥n
   - Filtros y b√∫squeda

5. **Exportaci√≥n**
   - Formatos m√∫ltiples (CSV, JSON, Excel)
   - Configuraci√≥n personalizada
   - Compartir resultados

### Inicio R√°pido con GUI

1. **Iniciar la aplicaci√≥n:**
   ```bash
   python src/gui.py
   ```

2. **Configurar par√°metros:**
   - Dominio objetivo
   - N√∫mero de p√°ginas
   - Delays entre requests

3. **Cargar keywords:**
   - Desde archivo o manualmente
   - Generar con Google Suggest

4. **Ejecutar scraping:**
   - Monitorear progreso
   - Ver resultados en tiempo real

5. **Exportar resultados:**
   - Elegir formato
   - Descargar archivos

## üìä API

### Uso Program√°tico

```python
from stealth_scraper import StealthSerpScraper
from config.settings import config

# Inicializar scraper
scraper = StealthSerpScraper(config)

# Scraping b√°sico
keywords = ["seo", "marketing digital"]
results = scraper.batch_position_check(keywords, "example.com", pages=1)

# Google Suggest
suggestions = scraper.google_suggest_scraper("marketing", country="ES", language="es")

# An√°lisis
from utils import ResultsAnalyzer
analyzer = ResultsAnalyzer()
stats = analyzer.analyze_file("data/positions.csv")
```

### Ejemplos de Integraci√≥n

```python
# Integraci√≥n con base de datos
import sqlite3
from stealth_scraper import StealthSerpScraper

def save_to_database(results):
    conn = sqlite3.connect('positions.db')
    cursor = conn.cursor()
    
    for result in results:
        cursor.execute('''
            INSERT OR REPLACE INTO positions 
            (keyword, position, domain, timestamp) 
            VALUES (?, ?, ?, datetime('now'))
        ''', (result['keyword'], result['position'], result['domain']))
    
    conn.commit()
    conn.close()

# Uso
scraper = StealthSerpScraper(config)
results = scraper.batch_position_check(keywords, target_domain, pages=1)
save_to_database(results)
```

## üö® Troubleshooting

### Problemas Comunes

#### ChromeDriver Issues
```bash
# Actualizar undetected-chromedriver
pip install --upgrade undetected-chromedriver

# Verificar versi√≥n de Chrome
google-chrome --version

# Forzar reinstalaci√≥n
pip uninstall undetected-chromedriver
pip install undetected-chromedriver
```

#### Proxy Issues
```bash
# Probar proxies individualmente
python -c "from src.utils import ProxyTester; ProxyTester.test_proxy('tu-proxy:puerto')"

# Probar lista completa
python -c "from src.utils import ProxyTester; ProxyTester.test_proxy_list(['proxy1:port', 'proxy2:port'])"
```

#### Rate Limiting
- **S√≠ntomas**: CAPTCHAs, bloqueos, resultados vac√≠os
- **Soluciones**:
  - Aumentar delays en `.env`
  - Usar m√°s proxies
  - Reducir p√°ginas por keyword
  - Cambiar user-agents

#### Performance Lenta
- **Causas**: Muchas keywords, proxies lentos, delays altos
- **Optimizaciones**:
  - Usar proxies premium
  - Ajustar delays seg√∫n necesidad
  - Scrapear solo 1 p√°gina por defecto

### Logs y Debugging

```bash
# Ver logs en tiempo real
tail -f logs/scraper.log

# Nivel de debug
LOG_LEVEL=DEBUG
```

## ‚ùì FAQ

### ¬øEs legal usar este scraper?
**S√≠**, para investigaci√≥n leg√≠tima y an√°lisis SEO. Respetar siempre:
- `robots.txt` de los sitios
- T√©rminos de servicio
- No sobrecargar servidores

### ¬øCu√°ntas keywords puedo scrapear?
- **Sin proxies**: 50-100 keywords/d√≠a
- **Con proxies**: 500-1000 keywords/d√≠a
- **Masivo**: 5000+ keywords/d√≠a (con infraestructura adecuada)

### ¬øC√≥mo evitar bloqueos?
1. Usar proxies rotativos
2. Configurar delays apropiados (5-15s)
3. Rotar user-agents
4. Monitorear logs constantemente

### ¬øQu√© formatos de exportaci√≥n soporta?
- CSV (compatible con Excel, Google Sheets)
- JSON (para an√°lisis program√°tico)
- An√°lisis estad√≠stico (m√©tricas autom√°ticas)

### ¬øPuedo programar scraping autom√°tico?
S√≠, usando cron jobs o servicios como:
```bash
# Ejemplo de cron job diario
0 2 * * * cd /ruta/al/scraper && python src/main.py --keyword-file keywords.txt --domain ejemplo.com
```

## ü§ù Contribuci√≥n

### C√≥mo Contribuir

1. **Fork** el proyecto
2. **Crea una rama** (`git checkout -b feature/nueva-funcionalidad`)
3. **Commit** cambios (`git commit -am 'A√±adir nueva funcionalidad'`)
4. **Push** rama (`git push origin feature/nueva-funcionalidad`)
5. **Crea Pull Request**

### Estructura del C√≥digo

```
scraper-keyword-position/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # CLI principal
‚îÇ   ‚îú‚îÄ‚îÄ gui.py              # Interfaz gr√°fica
‚îÇ   ‚îú‚îÄ‚îÄ stealth_scraper.py  # Motor de scraping
‚îÇ   ‚îî‚îÄ‚îÄ utils.py            # Utilidades
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ settings.py         # Gestor de configuraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ .env               # Variables de entorno
‚îú‚îÄ‚îÄ data/                  # Resultados
‚îú‚îÄ‚îÄ logs/                  # Logs de actividad
‚îî‚îÄ‚îÄ tests/                 # Tests unitarios
```

### Desarrollo

```bash
# Configurar entorno de desarrollo
git clone <repo>
cd scraper-keyword-position
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Dependencias de desarrollo

# Ejecutar tests
python -m pytest tests/

# Verificar c√≥digo
flake8 src/
black src/
```

## üîÑ Gesti√≥n de Proxies

### Proxies Gratuitos

El proyecto incluye una lista de proxies gratuitos en `proxies_gratuitos.txt` con opciones de:
- **Alta velocidad** - Para scraping b√°sico
- **Diferentes regiones** - Europa, Estados Unidos, Asia
- **M√∫ltiples protocolos** - HTTP, HTTPS, SOCKS5

### Configuraci√≥n R√°pida

1. **Cargar proxies desde archivo:**
   ```bash
   # En la GUI, usar el bot√≥n "Cargar desde Archivo" en la pesta√±a de configuraci√≥n
   # O editar manualmente config/.env
   ```

2. **Probar proxies:**
   ```bash
   python -c "from src.utils import ProxyTester; ProxyTester.test_proxy_list(['51.158.68.68:8811', '188.166.59.17:8888'])"
   ```

3. **Configurar en .env:**
   ```env
   PROXY_1=51.158.68.68:8811
   PROXY_2=188.166.59.17:8888
   PROXY_3=185.162.251.147:80
   ```

### Recomendaciones de Uso

- **Scraping b√°sico**: 3-5 proxies de alta velocidad
- **Scraping masivo**: 10+ proxies con rotaci√≥n autom√°tica
- **Monitoreo**: Revisar logs frecuentemente por bloqueos
- **Actualizaci√≥n**: Los proxies gratuitos cambian frecuentemente

## üìÑ Licencia

Este proyecto es de c√≥digo abierto para uso educativo y de investigaci√≥n. El usuario es responsable de cumplir con los t√©rminos de servicio de los sitios web scrapeados.

**‚ö†Ô∏è Disclaimer**: Este scraper es para uso educativo y de investigaci√≥n. El uso responsable es responsabilidad del usuario.

## üìû Soporte

### Canales de Ayuda

1. **Documentaci√≥n**: Revisa este README y los comentarios en el c√≥digo
2. **Issues**: Reporta bugs en GitHub Issues
3. **Comunidad**: √önete a nuestro Discord/Telegram
4. **Email**: soporte@tudominio.com

### Informaci√≥n para Reportar Issues

Al reportar un problema, incluye:
- Versi√≥n de Python y Chrome
- Configuraci√≥n usada (sin credenciales)
- Logs relevantes
- Pasos para reproducir
- Comportamiento esperado vs actual

---

**¬øTe gusta este proyecto? ‚≠ê Dale una estrella en GitHub!**
